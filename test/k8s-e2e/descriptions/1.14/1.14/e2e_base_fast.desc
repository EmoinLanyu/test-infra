[Conformance]
[k8s.io] [Feature:Example] [k8s.io] Downward API should create a pod that prints his name and namespace
[k8s.io] [Feature:Example] [k8s.io] Liveness liveness pods should be automatically restarted
[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s.
[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support pod.Spec.SecurityContext.SupplementalGroups
[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp alpha runtime/default annotation [Feature:Seccomp]
[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp alpha unconfined annotation on the container [Feature:Seccomp]
[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp alpha unconfined annotation on the pod [Feature:Seccomp]
[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp default which is unconfined [Feature:Seccomp]
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with readonly rootfs when readOnlyRootFilesystem=true [NodeConformance]
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance]
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when not explicitly set and uid != 0 [NodeConformance]
[k8s.io] Sysctls [NodeFeature:Sysctls] should not launch unsafe, but not explicitly enabled sysctls on the node
[k8s.io] Sysctls [NodeFeature:Sysctls] should reject invalid sysctls
[k8s.io] Sysctls [NodeFeature:Sysctls] should support sysctls
[k8s.io] Sysctls [NodeFeature:Sysctls] should support unsafe sysctls which are actually whitelisted
[sig-api-machinery] Initializers [Feature:Initializers] should be invisible to controllers by default
[sig-api-machinery] Initializers [Feature:Initializers] will be set to nil if a patch removes the last pending initializer
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should adopt matching orphans and release non-matching pods
[sig-auth] [Feature:NodeAuthorizer] A node shouldn't be able to create another node
[sig-auth] [Feature:NodeAuthorizer] A node shouldn't be able to delete another node
[sig-auth] [Feature:NodeAuthorizer] Getting a non-existent configmap should exit with the Forbidden error, not a NotFound error
[sig-auth] [Feature:NodeAuthorizer] Getting a non-existent secret should exit with the Forbidden error, not a NotFound error
[sig-auth] [Feature:NodeAuthorizer] Getting a secret for a workload the node has access to should succeed
[sig-auth] [Feature:NodeAuthorizer] Getting an existing configmap should exit with the Forbidden error
[sig-auth] [Feature:NodeAuthorizer] Getting an existing secret should exit with the Forbidden error
[sig-cli] Kubectl client [k8s.io] Kubectl apply apply set/view last-applied
[sig-cli] Kubectl client [k8s.io] Kubectl apply should apply a new configuration to an existing RC
[sig-cli] Kubectl client [k8s.io] Kubectl apply should reuse port when apply to an existing SVC
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info dump should check if cluster-info dump succeeds
[sig-cli] Kubectl client [k8s.io] Kubectl copy should copy a file from a running Pod
[sig-cli] Kubectl client [k8s.io] Kubectl create quota should create a quota with scopes
[sig-cli] Kubectl client [k8s.io] Kubectl create quota should create a quota without scopes
[sig-cli] Kubectl client [k8s.io] Kubectl create quota should reject quota with invalid scopes
[sig-cli] Kubectl client [k8s.io] Kubectl run CronJob should create a CronJob
[sig-cli] Kubectl client [k8s.io] Proxy server should support --unix-socket=/path  [Conformance]
[sig-cli] Kubectl client [k8s.io] Simple pod should return command exit codes
[sig-cli] Kubectl client [k8s.io] Simple pod should support exec
[sig-cli] Kubectl client [k8s.io] Simple pod should support exec through an HTTP proxy
[sig-cli] Kubectl client [k8s.io] Simple pod should support exec through kubectl proxy
[sig-cli] Kubectl client [k8s.io] Simple pod should support inline execution and attach
[sig-cli] Kubectl client [k8s.io] Simple pod should support port-forward
[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects a client request should support a client that connects, sends DATA, and disconnects
[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects a client request should support a client that connects, sends NO DATA, and disconnects
[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects NO client request should support a client that connects, sends DATA, and disconnects
[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 should support forwarding over websockets
[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects a client request should support a client that connects, sends DATA, and disconnects
[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects a client request should support a client that connects, sends NO DATA, and disconnects
[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects NO client request should support a client that connects, sends DATA, and disconnects
[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost should support forwarding over websockets
[sig-cluster-lifecycle] [Feature:BootstrapTokens] should delete the token secret when the secret expired
[sig-cluster-lifecycle] [Feature:BootstrapTokens] should not delete the token secret when the secret is not expired
[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not be able to proxy to cadvisor port 4194 using proxy subresource
[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not be able to proxy to the readonly kubelet port 10255 using proxy subresource
[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's multiple priority class scope (quota set to pod count: 2) against 2 pods with same priority classes.
[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (cpu, memory quota set) against a pod with same priority class.
[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (quota set to pod count: 1) against 2 pods with different priority class.
[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (quota set to pod count: 1) against 2 pods with same priority class.
[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (quota set to pod count: 1) against a pod with different priority class (ScopeSelectorOpExists).
[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (quota set to pod count: 1) against a pod with different priority class (ScopeSelectorOpNotIn).
[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (quota set to pod count: 1) against a pod with same priority class.
[sig-scheduling] ResourceQuota [Feature:ScopeSelectors] should verify ResourceQuota with best effort scope using scope-selectors.
[sig-scheduling] ResourceQuota [Feature:ScopeSelectors] should verify ResourceQuota with terminating scopes through scope selectors.
[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim with a storage class. [sig-storage]
[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim. [sig-storage]
[sig-storage] ConfigMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeFeature:FSGroup]
[sig-storage] ConfigMap should be consumable from pods in volume as non-root with FSGroup [NodeFeature:FSGroup]
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root with FSGroup [NodeFeature:FSGroup]
[sig-storage] Downward API volume should provide podname as non-root with fsgroup [NodeFeature:FSGroup]
[sig-storage] Downward API volume should provide podname as non-root with fsgroup and defaultMode [NodeFeature:FSGroup]
[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] files with FSGroup ownership should support (root,0644,tmpfs)
[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is non-root
[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is root
[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] nonexistent volume subPath should have the correct mode and owner using FSGroup
[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] volume on default medium should have the correct mode using FSGroup
[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] volume on tmpfs should have the correct mode using FSGroup
[sig-storage] HostPath should support r/w [NodeConformance]
[sig-storage] HostPath should support subPath [NodeConformance]
[sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and read from pod1
[sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and write from pod1
[sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set fsGroup for one pod
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set same fsGroup for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set fsGroup for one pod
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set same fsGroup for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1
[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1
[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set fsGroup for one pod
[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set same fsGroup for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1
[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1
[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set fsGroup for one pod
[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set same fsGroup for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and read from pod1
[sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and write from pod1
[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set fsGroup for one pod
[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set same fsGroup for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and read from pod1
[sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and write from pod1
[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set fsGroup for one pod
[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set same fsGroup for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and read from pod1
[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and write from pod1
[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set fsGroup for one pod
[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set same fsGroup for two pods simultaneously
[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
[sig-storage] PersistentVolumes-local [Volume type: blockfswithformat] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously
[sig-storage] PersistentVolumes-local [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and read from pod1
[sig-storage] Projected configMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeFeature:FSGroup]
[sig-storage] Projected configMap should be consumable from pods in volume as non-root with FSGroup [NodeFeature:FSGroup]
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root with FSGroup [NodeFeature:FSGroup]
[sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup [NodeFeature:FSGroup]
[sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup and defaultMode [NodeFeature:FSGroup]
[sig-storage] Projected secret should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance]
[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret [NodeConformance]
[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from docker hub [NodeConformance]
[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from gcr.io [NodeConformance]
[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull from private registry without secret [NodeConformance]
[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull image from invalid registry [NodeConformance]
[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull non-existing image from gcr.io [NodeConformance]
[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogOnError is set [NodeConformance]
[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogOnError is set [NodeConformance]
[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message from log output if TerminationMessagePolicy FallbackToLogOnError is set [NodeConformance]
[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message if TerminationMessagePath is set [NodeConformance]
[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance]
[k8s.io] PrivilegedPod [NodeConformance] should enable privileged commands
[k8s.io] Security Context When creating a container with runAsUser should run the container with uid 0 [NodeConformance]
[k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [NodeConformance]
[k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [NodeConformance]
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with readonly rootfs when readOnlyRootFilesystem=true [NodeConformance]
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance]
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when not explicitly set and uid != 0 [NodeConformance]
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when true [NodeConformance]
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [NodeConformance]
[sig-api-machinery] CustomResourceDefinition Watch CustomResourceDefinition Watch watch on custom resource definition objects
[sig-api-machinery] Garbage collector should delete jobs and pods created by cronjob
[sig-api-machinery] Garbage collector should orphan pods created by rc if deleteOptions.OrphanDependents is nil
[sig-api-machinery] Garbage collector should support cascading deletion of custom resources
[sig-api-machinery] Garbage collector should support orphan deletion of custom resources
[sig-api-machinery] Generated clientset should create pods, set the deletionTimestamp and deletionGracePeriodSeconds of the pod
[sig-api-machinery] Generated clientset should create v1beta1 cronJobs, delete cronJobs, watch cronJobs
[sig-api-machinery] Secrets should fail to create secret in volume due to empty secret key
[sig-api-machinery] Servers with support for API chunking should return chunks of results for list calls
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata
[sig-api-machinery] Servers with support for Table transformation should return chunks of table results for list calls
[sig-api-machinery] Servers with support for Table transformation should return generic metadata details across all namespaces for nodes
[sig-api-machinery] Servers with support for Table transformation should return pod details
[sig-api-machinery] Watchers should receive events on concurrent watches in same order
[sig-apps] CronJob should delete successful finished jobs with limit of one successful job
[sig-apps] CronJob should not emit unexpected warnings
[sig-apps] CronJob should remove from active list jobs that have been deleted
[sig-apps] CronJob should replace jobs when ReplaceConcurrent
[sig-apps] CronJob should schedule multiple jobs concurrently
[sig-apps] Deployment deployment reaping should cascade to its replica sets and pods
[sig-apps] Deployment deployment should support rollback
[sig-apps] Deployment iterative rollouts should eventually progress
[sig-apps] Deployment test Deployment ReplicaSet orphaning and adoption regarding controllerRef
[sig-apps] DisruptionController evictions: enough pods, absolute => should allow an eviction
[sig-apps] DisruptionController evictions: enough pods, replicaSet, percentage => should allow an eviction
[sig-apps] DisruptionController evictions: maxUnavailable allow single eviction, percentage => should allow an eviction
[sig-apps] DisruptionController evictions: maxUnavailable deny evictions, integer => should not allow an eviction
[sig-apps] DisruptionController evictions: no PDB => should allow an eviction
[sig-apps] DisruptionController evictions: too few pods, absolute => should not allow an eviction
[sig-apps] DisruptionController evictions: too few pods, replicaSet, percentage => should not allow an eviction
[sig-apps] DisruptionController should create a PodDisruptionBudget
[sig-apps] DisruptionController should update PodDisruptionBudget status
[sig-apps] Job should adopt matching orphans and release non-matching pods
[sig-apps] Job should delete a job
[sig-apps] Job should exceed active deadline
[sig-apps] Job should exceed backoffLimit
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted
[sig-apps] Job should run a job to completion when tasks sometimes fail and are not locally restarted
[sig-apps] Job should run a job to completion when tasks succeed
[sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota
[sig-auth] Certificates API should support building a client with a CSR
[sig-auth] PodSecurityPolicy should forbid pod creation when no PSP is available
[sig-auth] ServiceAccounts should ensure a single API token exists
[sig-cli] Kubectl client [k8s.io] Proxy server should support --unix-socket=/path  [Conformance]
[sig-instrumentation] Cadvisor should be healthy on every node.
[sig-instrumentation] MetricsGrabber should grab all metrics from API server.
[sig-instrumentation] MetricsGrabber should grab all metrics from a ControllerManager.
[sig-instrumentation] MetricsGrabber should grab all metrics from a Kubelet.
[sig-instrumentation] MetricsGrabber should grab all metrics from a Scheduler.
[sig-network] DNS should provide DNS for pods for Hostname and Subdomain
[sig-network] DNS should support configurable pod resolv.conf
[sig-network] NetworkPolicy NetworkPolicy between server and client should allow ingress access on one named port [Feature:NetworkPolicy]
[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce multiple, stacked policies with overlapping podSelectors [Feature:NetworkPolicy]
[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on NamespaceSelector [Feature:NetworkPolicy]
[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on PodSelector [Feature:NetworkPolicy]
[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on Ports [Feature:NetworkPolicy]
[sig-network] NetworkPolicy NetworkPolicy between server and client should support a 'default-deny' policy [Feature:NetworkPolicy]
[sig-network] NetworkPolicy NetworkPolicy between server and client should support allow-all policy [Feature:NetworkPolicy]
[sig-network] Networking should provide unchanging, static URL paths for kubernetes api services
[sig-network] Services should be able to change the type from ClusterIP to ExternalName
[sig-network] Services should be able to change the type from ExternalName to ClusterIP
[sig-network] Services should be able to change the type from ExternalName to NodePort
[sig-network] Services should be able to change the type from NodePort to ExternalName
[sig-network] Services should be able to switch session affinity for NodePort service
[sig-network] Services should be able to switch session affinity for service with type clusterIP
[sig-network] Services should be able to update NodePorts with two same port numbers but different protocols
[sig-network] Services should check NodePort out-of-range
[sig-network] Services should create endpoints for unready pods
[sig-network] Services should have session affinity work for NodePort service
[sig-network] Services should have session affinity work for service with type clusterIP
[sig-network] Services should preserve source pod IP for traffic thru service cluster IP
[sig-network] Services should prevent NodePort collisions
[sig-network] Services should release NodePorts on delete
[sig-network] Services should use same NodePort with same port but different protocols
[sig-node] ConfigMap should fail to create configMap in volume due to empty configmap key
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied.
[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a configMap.
[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a pod.
[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a replica set.
[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a replication controller.
[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a secret.
[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a service.
[sig-scheduling] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated.
[sig-scheduling] ResourceQuota should verify ResourceQuota with best effort scope.
[sig-scheduling] ResourceQuota should verify ResourceQuota with terminating scopes.
[sig-storage] Dynamic Provisioning DynamicProvisioner allowedTopologies should create persistent volume in the zone specified in allowedTopologies of storageclass
[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : configmap
[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : projected
[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : secret
[sig-storage] PV Protection Verify that PV bound to a PVC is not removed immediately
[sig-storage] PVC Protection Verify that PVC in active use by a pod is not removed immediately
[sig-storage] PVC Protection Verify that scheduling of a pod that uses PVC that is being deleted fails and the pod becomes Unschedulable
[sig-storage] PersistentVolumes NFS when invoking the Recycle reclaim policy should test that a PV becomes Available and is clean after the PVC is deleted.
[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PV and a pre-bound PVC: test write access
[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and a pre-bound PV: test write access
[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and non-pre-bound PV: test write access
[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs should create a non-pre-bound PV and PVC: test write access
[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 2 PVs and 4 PVCs: test write access
[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 3 PVs and 3 PVCs: test write access
[sig-storage] PersistentVolumes-local  Pod with node different from PV's NodeAffinity should fail scheduling due to different NodeAffinity
[sig-storage] PersistentVolumes-local  Pod with node different from PV's NodeAffinity should fail scheduling due to different NodeSelector
[sig-storage] Volumes ConfigMap should be mountable
[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults
[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file
[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath
[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount
[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount
[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources
[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults
[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file
[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath
[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount
[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount
[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing single file
[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath
[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount
[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing single file
[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath
[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount
[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumes should be mountable
[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing single file
[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath
[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount
[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumes should be mountable
[k8s.io] Pods should support pod readiness gates [NodeFeature:PodReadinessGate]
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with readonly rootfs when readOnlyRootFilesystem=true [NodeConformance]
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance]
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when not explicitly set and uid != 0 [NodeConformance]
[sig-apps] DisruptionController evictions: enough pods, absolute => should allow an eviction
[sig-apps] DisruptionController evictions: enough pods, replicaSet, percentage => should allow an eviction
[sig-apps] DisruptionController evictions: maxUnavailable allow single eviction, percentage => should allow an eviction
[sig-apps] DisruptionController evictions: maxUnavailable deny evictions, integer => should not allow an eviction
[sig-apps] DisruptionController evictions: no PDB => should allow an eviction
[sig-apps] DisruptionController evictions: too few pods, absolute => should not allow an eviction
[sig-apps] DisruptionController evictions: too few pods, replicaSet, percentage => should not allow an eviction
[sig-cli] Kubectl client [k8s.io] Proxy server should support --unix-socket=/path [Conformance]